<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Bert/ELMo词向量及使用方法记录 | 天空的城</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="2020.07.04更新： 加入使用Transformers的Bert的例子，见github: pytorch-bert-elmo-example   2019.4.3更新：新增使用AllenNLP的ELMo做文本分类的例子说明：详见github: Pytorch-ELMo   Bert&#x2F;ELMo不同于word2vec、glove，属于上下文词向量模型, 可以很方便用于下游NLP任务中。  EL">
<meta property="og:type" content="article">
<meta property="og:title" content="Bert&#x2F;ELMo词向量及使用方法记录">
<meta property="og:url" content="http://shomy.top/2019/01/01/elmo-1/index.html">
<meta property="og:site_name" content="天空的城">
<meta property="og:description" content="2020.07.04更新： 加入使用Transformers的Bert的例子，见github: pytorch-bert-elmo-example   2019.4.3更新：新增使用AllenNLP的ELMo做文本分类的例子说明：详见github: Pytorch-ELMo   Bert&#x2F;ELMo不同于word2vec、glove，属于上下文词向量模型, 可以很方便用于下游NLP任务中。  EL">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://cdn.htliu.cn/blog/elmo-1/elmo.png">
<meta property="article:published_time" content="2019-01-01T15:59:36.000Z">
<meta property="article:modified_time" content="2021-12-16T15:11:45.656Z">
<meta property="article:author" content="ShomyLiu">
<meta property="article:tag" content="ELMo">
<meta property="article:tag" content="word emebdding">
<meta property="article:tag" content="pytorch">
<meta property="article:tag" content="tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://cdn.htliu.cn/blog/elmo-1/elmo.png">
  
  
    <link rel="icon" href="/image/favicon.ico">
  

  
  <link href="https://cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css" rel="stylesheet">
  
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link href="https://cdn.bootcss.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
  
<link rel="stylesheet" href="/css/style.css">

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-74368890-2', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  

  
  <div class="site-search header-inner">
    <div class="popup">
        <span class="search-icon fa fa-search"></span>
        <input type="text" id="local-search-input">
        <div id="local-search-result"></div>
        <span class="popup-btn-close">close</span>
    </div>
</div>



<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      
<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">天空的城</a>
      </h1>
      
    </div>
    <div id="header-menu">
      <nav id="main-nav">
        <ul>
        
          <li><a href="/"><i class="fa fa-home icon-setting"></i></a></li>
        
          <li><a href="/archives"><i class="fa fa-archive icon-setting"></i></a></li>
        
          <li><a href="/tags"><i class="fa fa-tag icon-setting"></i></a></li>
        
          <li><a href="/about"><i class="fa fa-user icon-setting"></i></a></li>
        
        
          <li><a href="javascript:;" class="popup-trigger"><i class="fa fa-search > icon-setting"></i></a></li>
        
        </ul>
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-elmo-1" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-meta">
    <a href="/2019/01/01/elmo-1/" class="article-date">
  <time datetime="2019-01-01T15:59:36.000Z" itemprop="datePublished">2019-01-01</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a>
  </div>


    
        <div class="counter-tag counter">
    <!-- 别忘记这个类名... post-title-link -->
    <span id="/2019/01/01/elmo-1/" class="leancloud_visitors article-hits post-title-link"
           data-flag-title="Bert/ELMo词向量及使用方法记录">
        次阅读
    </span>

  </div>

    

  </div>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Bert/ELMo词向量及使用方法记录
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
            <div id="toc" class="toc-article">
                <strong class="toc-title">文章目录</strong>
                <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#elmo%E7%AE%80%E4%BB%8B"><span class="post-toc-text">ELMo简介</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#pytorch%E7%89%88elmo-allennlp"><span class="post-toc-text">Pytorch版ELMo (Allennlp)</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#tensorflow%E7%89%88-elmo"><span class="post-toc-text">tensorflow版 ELMo</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#bert"><span class="post-toc-text">Bert</span></a></li></ol>
            </div>
        
        
        <blockquote>
<p>2020.07.04更新： 加入使用Transformers的Bert的例子，见github: <a target="_blank" rel="noopener" href="https://github.com/ShomyLiu/pytorch_bert_elmo_example">pytorch-bert-elmo-example</a></p>
</blockquote>
<blockquote>
<p>2019.4.3更新：新增使用AllenNLP的ELMo做文本分类的例子说明：详见github: <a target="_blank" rel="noopener" href="https://github.com/ShomyLiu/pytorch_bert_elmo_example">Pytorch-ELMo</a></p>
</blockquote>
<hr />
<p>Bert/ELMo不同于word2vec、glove，属于上下文词向量模型, 可以很方便用于下游NLP任务中。</p>
<ul>
<li>ELMo基于LSTM, 来自《Deep Contextualized Word Representations》</li>
<li>Bert基于Transformer, 来自《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》</li>
</ul>
<span id="more"></span>
<h1 id="elmo简介">ELMo简介</h1>
<p>ELMo使用双层BiLSTM来训练语言模型，创新是线性组合不同层的word vectors， 作为最终的word representation. 核心公式:</p>
<p><img src="http://cdn.htliu.cn/blog/elmo-1/elmo.png" /></p>
<ul>
<li>第一层是普通的word embedding 可以用wrod2vec或者glove来得到，或者使用character level得到token embedding》 这部分是general embedding，上下文无关。文中使用的character level的CNN+Highway.</li>
<li>后面连接两个biLSTM 去encode 输入（同时也有残差连接), 每一层LSTM得到的输出（隐状态) 作为每个词的上下文相关的word vectors.</li>
<li>这样每个词就会有（L+1）个词向量，L为biLSTM的层数.</li>
<li>词向量的线性组合，针对不同的任务，不同层的向量做不同的权重加和。 其中权重s是一个维度为L的vector，参与训练。</li>
</ul>
<p>因此ELMo的基本输入单元为句子，每个词没有固定的词向量，是根据词的上下文环境来动态产生当前词的词向量，常见的场景可以较好解决一词多义的问题，这一点跟word2vec与glove等通用词向量模型是不同的。</p>
<h2 id="pytorch版elmo-allennlp">Pytorch版ELMo (Allennlp)</h2>
<p>这个是官方的ELMo的版本，基于Allennlp或者pytorch。<a target="_blank" rel="noopener" href="https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md#using-elmo-as-a-pytorch-module-to-train-a-new-model">官方文档</a>给的example有些迷惑，这里记录一番。 Allennlp的ELMo的API为<code>allennlp.modules.elmo.Elmo</code>, 完整的输入参数参见<a target="_blank" rel="noopener" href="https://github.com/allenai/allennlp/blob/master/allennlp/modules/elmo.py#L34">文档</a>，几个比较重要的如下:</p>
<ul>
<li>options_file : ELMo JSON options file</li>
<li>weight_file : ELMo hdf5 weight file</li>
<li>num_output_representations: The number of ELMo representation layers to output.</li>
<li>requires_grad: optional, If True, compute gradient of ELMo parameters for fine tuning.</li>
</ul>
<p>其中一个关键的参数为<code>num_output_reprensenations</code> 文档描述不太清楚，具体可以看见在github提的<a target="_blank" rel="noopener" href="https://github.com/allenai/allennlp/issues/2245">issue</a>和<a target="_blank" rel="noopener" href="https://github.com/allenai/allennlp/pull/2256">pr</a>。 这个num_output_representation 一开始理解这个参数是输出三层中前几层，但是其实并不是这样，因为这个参数的取值可以是任意正整数。 我们知道ELMo文章中最后词向量表示即公式1计算的是三层表示的线性加权，针对不同的任务，可能会有不同的加权比例。 因此<code>num_output_representation</code> 表示输出多种线性加权的词向量，即多个公式1产生的词向量。 而Allennlp的ELMo 并不直接提供中间的三层输出(char-cnn, lstm-1, lstm-2)，不过可以通过稍微修改源代码的方法获得。</p>
<p>再看例子如下： <figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from allennlp.modules.elmo <span class="built_in">import</span> Elmo, batch_to_ids</span><br><span class="line"></span><br><span class="line"><span class="attr">options_file</span> = <span class="string">&quot;options.json&quot;</span>  <span class="comment"># 配置文件地址 </span></span><br><span class="line"><span class="attr">weight_file</span> = <span class="string">&quot;weights.hdf5&quot;</span> <span class="comment"># 权重文件地址</span></span><br><span class="line"><span class="comment"># 这里的1表示产生一组线性加权的词向量。</span></span><br><span class="line"><span class="comment"># 如果改成2 即产生两组不同的线性加权的词向量。</span></span><br><span class="line"><span class="attr">elmo</span> = Elmo(options_file, weight_file, <span class="number">1</span>, <span class="attr">dropout=0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># use batch_to_ids to convert sentences to character ids</span></span><br><span class="line"><span class="attr">sentence_lists</span> = [<span class="string">&quot;I have a dog&quot;</span>, <span class="string">&quot;How are you , today is Monday&quot;</span>,<span class="string">&quot;I am fine thanks&quot;</span>]</span><br><span class="line"><span class="attr">character_ids</span> = batch_to_ids(sentences_lists)</span><br><span class="line"></span><br><span class="line"><span class="attr">embeddings</span> = elmo(character_ids)[&#x27;elmo_representations&#x27;]</span><br></pre></td></tr></table></figure> 最终<code>len(embeddings) == 1</code>，里面的一个元素则是论文中公式1产生的ELMo的词向量, shape为<code>[batch_size, max_length, 1024]</code>, 即:[3, 9, 1024], (根据配置文件不同，最后维度可能不同, 参考<a target="_blank" rel="noopener" href="https://allennlp.org/elmo">预训练权重</a>)。</p>
<p>如果上述参数1改为2，则表示产生两个权重不同的ELMo词向量，可以用于不同任务的词向量。</p>
<p>一般单任务下，我们直接使用1就可以了。</p>
<h2 id="tensorflow版-elmo">tensorflow版 ELMo</h2>
<p>首先载入ELMo的方法很简单，直接load tfhub即可:</p>
<figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow_hub <span class="keyword">as</span> hub</span><br><span class="line"><span class="title">url</span> = <span class="string">&quot;https://tfhub.dev/google/elmo/2&quot;</span></span><br><span class="line"><span class="title">embed</span> = hub.<span class="type">Module</span>(url)</span><br></pre></td></tr></table></figure>
<p>调用方法也很简单: <figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">sentence_lists</span> = [<span class="string">&quot;I have a dog&quot;</span>, <span class="string">&quot;How are you , today is Monday&quot;</span>,<span class="string">&quot;I am fine thanks&quot;</span>]</span><br><span class="line"><span class="attr">output</span> = embed(sentence_lists, as_dict=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure> 输出的结果如下: <figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">&#x27;default&#x27;</span>: &lt;tf.Tensor <span class="string">&#x27;module_apply_default_6/truediv:0&#x27;</span> shape=(3, 1024) <span class="attribute">dtype</span>=float32&gt;,</span><br><span class="line">    <span class="string">&#x27;elmo&#x27;</span>: &lt;tf.Tensor <span class="string">&#x27;module_apply_default_6/aggregation/mul_3:0&#x27;</span> shape=(3, 9, 1024) <span class="attribute">dtype</span>=float32&gt;,</span><br><span class="line">    <span class="string">&#x27;lstm_outputs1&#x27;</span>: &lt;tf.Tensor <span class="string">&#x27;module_apply_default_6/concat:0&#x27;</span> shape=(3, ?, 1024) <span class="attribute">dtype</span>=float32&gt;,</span><br><span class="line">    <span class="string">&#x27;lstm_outputs2&#x27;</span>: &lt;tf.Tensor <span class="string">&#x27;module_apply_default_6/concat_1:0&#x27;</span> shape=(3, ?, 1024) <span class="attribute">dtype</span>=float32&gt;,</span><br><span class="line">    <span class="string">&#x27;sequence_len&#x27;</span>: &lt;tf.Tensor <span class="string">&#x27;module_apply_default_6/Sum:0&#x27;</span> shape=(3,) <span class="attribute">dtype</span>=int32&gt;,</span><br><span class="line">    <span class="string">&#x27;word_emb&#x27;</span>: &lt;tf.Tensor <span class="string">&#x27;module_apply_default_6/bilm/Reshape_1:0&#x27;</span> shape=(3, 9, 512) <span class="attribute">dtype</span>=float32&gt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>解释下这些字段的意思:</p>
<ul>
<li><code>word_emb</code>: ELMo的最开始一层的基于character的word embedding, shape为<code>[batch_size, max_length, 512]</code></li>
<li><code>lstm_outpus1/2</code>: ELMo中的第一层和第二层LSTM的隐状态输出，shape同样为 <code>[batch_size, max_length, 1024]</code></li>
<li><code>elmo</code>: 对应文章中的公式1，每个词的输入层(word_emb)，第一层LSTM输出，第二层LSTM输出的线性加权之后的最终的词向量，shape为<code>[batch_size, max_length, 1024]</code>，此外这个线性权重是可训练的。</li>
<li><code>default</code>: 前面得到的均为word级别的向量，这个选项给出了简单使用mean-pooling求的句子级别的向量，即将上述<code>elmo</code>的所有词取平均，方便后续下游任务。</li>
<li><code>sequence_len</code> 输入中每个句子的长度</li>
</ul>
<p>一般情况使用<code>output ['elmo']</code> 也就是Allennlp版本的<code>elmo_representations</code>， 即可得到每个词的ELMo 词向量，即可用于后续的任务，比如分类等。</p>
<h1 id="bert">Bert</h1>
<p>Bert使用Transformer来代替LSTM， 具体原理这里不再赘述了，网上非常多的资料，给出几个:</p>
<ul>
<li>讲解理论
<ul>
<li>http://jalammar.github.io/illustrated-transformer/</li>
<li>https://www.youtube.com/watch?v=ugWDIIOHtPA</li>
<li>【全面拥抱Transformer】https://zhuanlan.zhihu.com/p/54743941</li>
<li>【[整理] 聊聊 Transformer】https://zhuanlan.zhihu.com/p/47812375</li>
<li>http://jalammar.github.io/illustrated-bert/</li>
<li>https://www.youtube.com/watch?v=Bywo7m6ySlk</li>
</ul></li>
<li>实现介绍
<ul>
<li>https://nlp.seas.harvard.edu/2018/04/03/attention.html</li>
<li>调用fine-tuning: https://github.com/huggingface/transformers</li>
<li>pre-trained : https://github.com/google-research/bert</li>
<li>pre-trained: https://github.com/dbiir/UER-py</li>
</ul></li>
</ul>

      
    </div>


    

    
	    <div class="article-footer-copyright">
<!--<p>本文作者: shomy 发表于 <a href="http://shomy.top" target="_blank">个人博客</a></p> -->
<p>
本文标题: Bert/ELMo词向量及使用方法记录<br/>
</p>
<p>
发布时间: 2019-01-01, 23:59:36<br/>
<p>
<p>
最后更新: 2021-12-16, 23:11:45<br/>
<p>
本文链接: <a href="/2019/01/01/elmo-1/" target="_blank">http://shomy.top/2019/01/01/elmo-1/</a>
</p>
<p>非商业转载请注明作者及出处。商业转载请联系<a href="mailto:shomyliu@gmail.com">作者</a>本人。</p>
</div>

    

    <footer class="article-footer">
      
        <a href="http://shomy.top/2019/01/01/elmo-1/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ELMo/" rel="tag">ELMo</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/word-emebdding/" rel="tag">word emebdding</a></li></ul>

    </footer>
    
	    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/02/27/2018-review/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          2018
        
      </div>
    </a>
  
  
    <a href="/2018/12/31/factorization-machine/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Factorization Machine笔记及Pytorch 实现</div>
    </a>
  
</nav>

  
</article>




<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
<script>
  var disqus_shortname = 'shomy';
  
  var disqus_url = 'http://shomy.top/2019/01/01/elmo-1/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  }());
</script>
</section>




</section>
      </div>
    </div>
    
    
<a id="rocket" href="#top" ></a>

<script src="https://cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script>


  <script src="http://cdn.htliu.cn/static/js/leancloud.js"></script>
<script>AV.initialize("k883AKdzaz5jccQnH1eIrG2r-gzGzoHsz", "4HBQOmslEp3VdwK1SGL8vB9Y");</script>

<script src="/js/Counter.js"></script>





<script id="dsq-count-scr" src="//shomy.disqus.com/count.js" async></script>
 


  <script src="https://cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js"></script>
  
<script src="/js/fancybox.js"></script>




  
<script src="/js/search.js"></script>



<script src="/js/script.js"></script>



  </div>
</body>
</html>
