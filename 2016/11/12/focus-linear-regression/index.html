<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>机器学习基石笔记(4)-线性回归 | 天空的城</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="11.25日更新: Linear-Regression练习    前面的学习基本都是介绍机器学习可以学习的原因。这里开始讲如何学习。也就是如何选择算法去找到最好的hypothesis.接下来几篇将会介绍线性模型，包括前面的PLA(感知机), 以及这一节的Linear Regression(线性回归)，还有下一节的Logistic Regression(逻辑回归)，以及它们的联系。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习基石笔记(4)-线性回归">
<meta property="og:url" content="http://shomy.top/2016/11/12/focus-linear-regression/index.html">
<meta property="og:site_name" content="天空的城">
<meta property="og:description" content="11.25日更新: Linear-Regression练习    前面的学习基本都是介绍机器学习可以学习的原因。这里开始讲如何学习。也就是如何选择算法去找到最好的hypothesis.接下来几篇将会介绍线性模型，包括前面的PLA(感知机), 以及这一节的Linear Regression(线性回归)，还有下一节的Logistic Regression(逻辑回归)，以及它们的联系。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://cdn.htliu.cn/ml04-1.png">
<meta property="og:image" content="http://cdn.htliu.cn/ml04-2.png">
<meta property="article:published_time" content="2016-11-12T08:54:49.000Z">
<meta property="article:modified_time" content="2021-12-16T15:51:33.872Z">
<meta property="article:author" content="ShomyLiu">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="linear regression">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://cdn.htliu.cn/ml04-1.png">
  
  
    <link rel="icon" href="/image/favicon.ico">
  

  
  <link href="https://cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css" rel="stylesheet">
  
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link href="https://cdn.bootcss.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
  
<link rel="stylesheet" href="/css/style.css">

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-74368890-2', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  

  
  <div class="site-search header-inner">
    <div class="popup">
        <span class="search-icon fa fa-search"></span>
        <input type="text" id="local-search-input">
        <div id="local-search-result"></div>
        <span class="popup-btn-close">close</span>
    </div>
</div>



<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      
<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">天空的城</a>
      </h1>
      
    </div>
    <div id="header-menu">
      <nav id="main-nav">
        <ul>
        
          <li><a href="/"><i class="fa fa-home icon-setting"></i></a></li>
        
          <li><a href="/archives"><i class="fa fa-archive icon-setting"></i></a></li>
        
          <li><a href="/tags"><i class="fa fa-tag icon-setting"></i></a></li>
        
          <li><a href="/about"><i class="fa fa-user icon-setting"></i></a></li>
        
        
          <li><a href="javascript:;" class="popup-trigger"><i class="fa fa-search > icon-setting"></i></a></li>
        
        </ul>
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-focus-linear-regression" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-meta">
    <a href="/2016/11/12/focus-linear-regression/" class="article-date">
  <time datetime="2016-11-12T08:54:49.000Z" itemprop="datePublished">2016-11-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a>
  </div>


    
        <div class="counter-tag counter">
    <!-- 别忘记这个类名... post-title-link -->
    <span id="/2016/11/12/focus-linear-regression/" class="leancloud_visitors article-hits post-title-link"
           data-flag-title="机器学习基石笔记(4)-线性回归">
        次阅读
    </span>

  </div>

    

  </div>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      机器学习基石笔记(4)-线性回归
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
            <div id="toc" class="toc-article">
                <strong class="toc-title">文章目录</strong>
                <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="post-toc-text">模型</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="post-toc-text">小结</span></a></li></ol>
            </div>
        
        
        <blockquote>
<p>11.25日更新: <a target="_blank" rel="noopener" href="https://github.com/ShomyLiu/stat-learn/blob/master/linear-model/linear-regression/linear-regression.ipynb">Linear-Regression练习</a></p>
</blockquote>
<hr />
<blockquote>
<p>前面的学习基本都是介绍机器学习可以学习的原因。这里开始讲如何学习。也就是如何选择算法去找到最好的hypothesis.接下来几篇将会介绍线性模型，包括前面的PLA(感知机), 以及这一节的Linear Regression(线性回归)，还有下一节的Logistic Regression(逻辑回归)，以及它们的联系。 <span id="more"></span></p>
</blockquote>
<h2 id="模型">模型</h2>
<p>仍然拿申请信用卡为例，PLA算法是用来判断给不给信用卡的，而如果我们想要知道给多少额度呢？ 这个时候，我们可以就可以直接使用<span class="math inline">\(score=W^TX\)</span>值了，而不用sign函数了。因此就可以很容易的定义出Linear Regression的模型:<span class="math display">\[ y= \sum\limits_{i=0}^{d}w_ix_i=W^TX\]</span>是不是跟PLA:<span class="math inline">\(y=sign( \sum\limits_{i=0}^{d}w_ix_i)\)</span>很像。注意，这里面的<span class="math inline">\(W,X\)</span>都是<span class="math inline">\((d+1)\)</span>维的列向量。下面就是开始找到一个算法来找到最好的h即可。</p>
<p>我们的数据集是<span class="math inline">\(D=\{ (x_1, y_1),...,(x_n,y_n)\}\)</span>，那么最好的<span class="math inline">\(h(x_i)\)</span>得到的<span class="math inline">\(\hat{y_i}\)</span>应该与<span class="math inline">\(y_i\)</span>很接近，这样自然而然就可以想到使用如下的损失函数作为评估每个h的标准:<span class="math inline">\(err(\hat{y},y) = (\hat{y}-y)^2\)</span>那么训练数据集<span class="math inline">\(D\)</span>对应的损失就应该是<span class="math display">\[err(w) = E_{in}(w) = \frac{1}{N}\sum\limits_{i=0}^{N}(\hat{y}_i-y_i)^2=\frac{1}{N}\sum\limits_{i=0}^{N}(w^Tx_i-y_i)^2\]</span></p>
<p>下面的事情就是如何求出使得这个<span class="math inline">\(err\)</span>最小的<span class="math inline">\(w\)</span>。先化简为矩阵的形式:<span class="math display">\[\begin{aligned} E_{in}(\color{blue}{w}) &amp;= \frac{1}{N}\sum_{n=1}^{N}(\color{blue}{w^T}\color{red}{x_n}-\color{purple}{y_n})^2=\frac{1}{N}\sum_{n=1}^{N}(\color{red}{x_n^T}\color{blue}{w}-\color{purple}{y_n})^2 \\\ &amp;=\frac{1}{N}\begin{Vmatrix} \color{red}{x_1^T}\color{blue}{w}-\color{purple}{y_1}\\\ \color{red}{x_2^T}\color{blue}{w}-\color{purple}{y_2}\\\ ...\\\ \color{red}{x_N^T}\color{blue}{w}-\color{purple}{y_N} \end{Vmatrix}^2 \\\ &amp;=\frac{1}{N}\begin{Vmatrix} \color{red}{\begin{bmatrix} --x_1^T--\\\ --x_2^T--\\\ ...\\\ --x_N^T-- \end{bmatrix}} \color{blue}{w} - \color{purple}{\begin{bmatrix} y_1\\\ y_2\\\ ...\\\ y_3 \end{bmatrix}} \end{Vmatrix}^2 \\\ &amp;=\frac{1}{N}|| \underbrace{\color{red}{X}}_{N\times (d+1)}\;\;\; \underbrace{\color{blue}{w}}_{(d+1)\times 1} \; - \; \underbrace{\color{purple}{y}}_{N\times 1} ||^2 \end{aligned}\]</span></p>
<p>这样我们的目标就是:<span class="math inline">\(\min E_{in}(w) = \frac{1}{N}\begin{Vmatrix}{X}{w}-{y}\end{Vmatrix}^2\)</span>, 这个可以近似看成关于<span class="math inline">\(w\)</span>的二次函数，图像大致如下:<img src="http://cdn.htliu.cn/ml04-1.png" />这个是凸函数，因此我们可以通过求一阶导数然后为0的<span class="math inline">\(w\)</span>即可。这就是线性代数的范围了，类比二次函数即可，结果如下:</p>
<p><span class="math display">\[\nabla E_{in}(w)=\frac{2}{N}(X^TXw-w^TX^T)\]</span> 接下来的事情就简单了，让<span class="math inline">\(\nabla E_{in} = 0\)</span>, 可以得到我们的最优的参数<span class="math inline">\(w_{lin}\)</span>:</p>
<p><span class="math display">\[w_{lin} = (X^TX)^{-1}X^Ty\]</span>，我们一般把<span class="math inline">\((X^TX)^{-1}X^T\)</span>设为 pseudo-iinverse: <span class="math inline">\(X^{\dagger}\)</span>, 这样我们经常这样写:<span class="math inline">\(w_{lin}=X^{\dagger}y\)</span>。另外需要注意的，这是在<span class="math inline">\(X^TX\)</span>可逆的情况下，也就是<span class="math inline">\(X\)</span>满秩，如果不可逆的情况下，会使用其它方式定义，比如岭回归，就是对<span class="math inline">\(X^TX\)</span>进行修正，这里不多说。到此为止，我们求出了最优的<span class="math inline">\(w\)</span>, 这样再来一个新的数据点<span class="math inline">\(X\)</span>, 那么预测值就是:<span class="math display">\[\hat{y} = X w_{lin} = XX^{\dagger}y\]</span></p>
<p>现在，我们可以给出完整的线性回归算法了: <img src="http://cdn.htliu.cn/ml04-2.png" /></p>
<h2 id="小结">小结</h2>
<p>我们可以看到，线性回归模型并不是一次一次的迭代获得最优解的过程，而是一次求解就可以，这个就归功于损失函数有解析解。复杂度都集中在求逆的过程，效率还是很高的。</p>

      
    </div>


    

    
	    <div class="article-footer-copyright">
<!--<p>本文作者: shomy 发表于 <a href="http://shomy.top" target="_blank">个人博客</a></p> -->
<p>
本文标题: 机器学习基石笔记(4)-线性回归<br/>
</p>
<p>
发布时间: 2016-11-12, 16:54:49<br/>
<p>
<p>
最后更新: 2021-12-16, 23:51:33<br/>
<p>
本文链接: <a href="/2016/11/12/focus-linear-regression/" target="_blank">http://shomy.top/2016/11/12/focus-linear-regression/</a>
</p>
<p>非商业转载请注明作者及出处。商业转载请联系<a href="mailto:shomyliu@gmail.com">作者</a>本人。</p>
</div>

    

    <footer class="article-footer">
      
        <a href="http://shomy.top/2016/11/12/focus-linear-regression/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/linear-regression/" rel="tag">linear regression</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/" rel="tag">machine learning</a></li></ul>

    </footer>
    
	    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/11/16/logistic-regression-1/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          机器学习基石笔记(5)-逻辑回归
        
      </div>
    </a>
  
  
    <a href="/2016/10/22/hexo-markdown-mathjax/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hexo下mathjax的转义问题</div>
    </a>
  
</nav>

  
</article>




<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
<script>
  var disqus_shortname = 'shomy';
  
  var disqus_url = 'http://shomy.top/2016/11/12/focus-linear-regression/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  }());
</script>
</section>




</section>
      </div>
    </div>
    
    
<a id="rocket" href="#top" ></a>

<script src="https://cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script>


  <script src="http://cdn.htliu.cn/static/js/leancloud.js"></script>
<script>AV.initialize("k883AKdzaz5jccQnH1eIrG2r-gzGzoHsz", "4HBQOmslEp3VdwK1SGL8vB9Y");</script>

<script src="/js/Counter.js"></script>





<script id="dsq-count-scr" src="//shomy.disqus.com/count.js" async></script>
 


  <script src="https://cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js"></script>
  
<script src="/js/fancybox.js"></script>




  
<script src="/js/search.js"></script>



<script src="/js/script.js"></script>



  </div>
</body>
</html>
