<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>PyTorch中NLLLoss|CrossEntropy|BCELoss记录 | 天空的城</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="在分类任务中，有几个常用的损失函数，包括NLLLoss, CrossEntropy以及BCELosss，内容比较基础， 这里以pytorch的函数为例，回顾下细节和使用方法作为记录。 NLLLoss">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch中NLLLoss|CrossEntropy|BCELoss记录">
<meta property="og:url" content="http://shomy.top/2021/05/21/torch-loss/index.html">
<meta property="og:site_name" content="天空的城">
<meta property="og:description" content="在分类任务中，有几个常用的损失函数，包括NLLLoss, CrossEntropy以及BCELosss，内容比较基础， 这里以pytorch的函数为例，回顾下细节和使用方法作为记录。 NLLLoss">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-05-21T13:07:55.000Z">
<meta property="article:modified_time" content="2021-12-16T15:11:45.674Z">
<meta property="article:author" content="ShomyLiu">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
  
  
    <link rel="icon" href="/image/favicon.ico">
  

  
  <link href="https://cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css" rel="stylesheet">
  
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link href="https://cdn.bootcss.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
  
<link rel="stylesheet" href="/css/style.css">

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-74368890-2', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  

  
  <div class="site-search header-inner">
    <div class="popup">
        <span class="search-icon fa fa-search"></span>
        <input type="text" id="local-search-input">
        <div id="local-search-result"></div>
        <span class="popup-btn-close">close</span>
    </div>
</div>



<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      
<header id="header">
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">天空的城</a>
      </h1>
      
    </div>
    <div id="header-menu">
      <nav id="main-nav">
        <ul>
        
          <li><a href="/"><i class="fa fa-home icon-setting"></i></a></li>
        
          <li><a href="/archives"><i class="fa fa-archive icon-setting"></i></a></li>
        
          <li><a href="/tags"><i class="fa fa-tag icon-setting"></i></a></li>
        
          <li><a href="/about"><i class="fa fa-user icon-setting"></i></a></li>
        
        
          <li><a href="javascript:;" class="popup-trigger"><i class="fa fa-search > icon-setting"></i></a></li>
        
        </ul>
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-torch-loss" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-meta">
    <a href="/2021/05/21/torch-loss/" class="article-date">
  <time datetime="2021-05-21T13:07:55.000Z" itemprop="datePublished">2021-05-21</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a>
  </div>


    
        <div class="counter-tag counter">
    <!-- 别忘记这个类名... post-title-link -->
    <span id="/2021/05/21/torch-loss/" class="leancloud_visitors article-hits post-title-link"
           data-flag-title="PyTorch中NLLLoss|CrossEntropy|BCELoss记录">
        次阅读
    </span>

  </div>

    

  </div>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      PyTorch中NLLLoss|CrossEntropy|BCELoss记录
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
            <div id="toc" class="toc-article">
                <strong class="toc-title">文章目录</strong>
                <ol class="post-toc"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#nllloss"><span class="post-toc-text">NLLLoss</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#crossentropy-loss"><span class="post-toc-text">CrossEntropy Loss</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#bceloss"><span class="post-toc-text">BCELoss</span></a></li></ol>
            </div>
        
        
        <p>在分类任务中，有几个常用的损失函数，包括<code>NLLLoss, CrossEntropy</code>以及<code>BCELosss</code>，内容比较基础， 这里以pytorch的函数为例，回顾下细节和使用方法作为记录。</p>
<h3 id="nllloss">NLLLoss</h3>
<span id="more"></span>
<p>NLLLoss其实就是负对数似然损失(Negative Log Likelihood Loss)，直接将最大化似然取负数，就是最小化损失了。取对数是将为了方便累加，一般用在多分类问题上，定义如下： <span class="math display">\[
nll(x, y)= -logx[y]
\]</span> 其中, x为输入的向量，维度为类别数目，可以理解为每一个类别的score，y为真实类别的标量。</p>
<p>举例，如果 <span class="math inline">\(x=[0.1, 0.3, 0.5], y=1\)</span> ，含义就是一共有3个类别，每一类的预测概率或者分数为0.1,0.3,0.5（这里未归一化）, 真实标签为1， 那么nll(x,y)=-log0.3.</p>
<p>不过在pytorch的实现中，并没有log，只有一个负号，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 两条数据，一共三类；每一类的score</span></span><br><span class="line">x = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.5</span>], [<span class="number">0.2</span>,<span class="number">0.4</span>,<span class="number">0.6</span>]])</span><br><span class="line">y = torch.tensor([<span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(F.nll_loss(x, y, reduction=<span class="string">&#x27;none&#x27;</span>))</span><br><span class="line"><span class="comment"># tensor([-0.5, -0.4])</span></span><br></pre></td></tr></table></figure>
<p>所以要想真正的使用NLLLoss的用在分类的话， 需要提前对输入取softmax和log，这样才是负对数似然损失。(但这其实就是cross_entropy了)</p>
<h3 id="crossentropy-loss">CrossEntropy Loss</h3>
<p>CrossEntropyLoss交叉熵损失函数应该是在分类任务中出现频次最多的损失函数了，其实就是上述NLLLoss的完整版，可以直接用在分类任务中。</p>
<p>即：对于输入x向量，首先进行softmax操作，得到归一化的每一类的概率，之后进行log操作，最后执行NLLLoss，也就是说，</p>
<p>CE就是 log_softmax 与 nll_loss的结合得到的损失函数，如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 两条数据，一共三类；每一类的score</span></span><br><span class="line">x = torch.tensor([[<span class="number">0.4</span>, <span class="number">0.3</span>, <span class="number">0.7</span>], [<span class="number">1.2</span>,<span class="number">0.4</span>,<span class="number">0.6</span>]])</span><br><span class="line"><span class="comment"># 两条数据的真实label。</span></span><br><span class="line">y = torch.tensor([<span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(F.cross_entropy(x, y, reduction=<span class="string">&#x27;none&#x27;</span>))</span><br><span class="line"><span class="comment"># tensor([0.8801, 1.4922])</span></span><br><span class="line">log_softmax_x = F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># tensor([[-1.1801, -1.2801, -0.8801],</span></span><br><span class="line"><span class="comment">#       [-0.6922, -1.4922, -1.2922]])</span></span><br><span class="line"><span class="built_in">print</span>(F.nll_loss(log_softmax_x, y, reduction=<span class="string">&#x27;none&#x27;</span>))</span><br><span class="line"><span class="comment"># tensor([0.8801, 1.4922])</span></span><br></pre></td></tr></table></figure>
<p>所以分类直接用 CrossEntropy Loss, 二分类多分类都可以</p>
<p>这里多说一句，交叉熵的具体计算如下： <span class="math display">\[
CE = -\sum_xp(x)\log q(x)
\]</span> 其中，p(x)为标签的真实分布，一般为one-hot向量，q(x)为模型预测预测的标签分布，因此这里可以理解为已经经过了softmax操作。</p>
<p>既然p的值只有真实标签的index的地方为1，其余都是0，因此0的地方就不用管了，只需要考虑真实标签的位置的值就行，也就是： <span class="math display">\[
CE = - log(q_*)
\]</span> 其中，<span class="math inline">\(q_*\)</span> 就是真实类别处的值，这就是上面的NLLLoss的形式(x[y])。所以CE就是logsoftmax + nllloss即可。</p>
<h3 id="bceloss">BCELoss</h3>
<p>BCELoss可以理解为二分类的CE，就 Binary Cross Entropy Loss, 也就是上面的Cross Entropy中的标签只有两个值0或者1，正负样本。</p>
<p>这样展开求和其实就是： <span class="math display">\[
BCE = -[ylogx + (1-y)log(1-x)]
\]</span> 所以在使用BCELoss之前，一般将x的值都计算到0-1之间(一般使用sigmoid)，即当前数据的标签为正样本的概率，另一类负样本的概率就是1-x了。这个其实就是逻辑回归的损失函数了。</p>
<p>举个例子:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 两条数据点，正样本的概率分别为0.3 和 0.9</span></span><br><span class="line">x = torch.tensor([[<span class="number">0.3</span>], [<span class="number">0.9</span>]]) </span><br><span class="line"><span class="comment"># 两条数据的真实标签，第一个数据为正样本，第二个为负样本</span></span><br><span class="line">y = torch.tensor([[<span class="number">1.</span>], [<span class="number">0.</span>]]) </span><br><span class="line"><span class="built_in">print</span>(F.binary_cross_entropy(x,y, reduction=<span class="string">&#x27;none&#x27;</span>))</span><br><span class="line"><span class="comment"># tensor([[1.2040], [2.3026]])</span></span><br></pre></td></tr></table></figure>
<p>以第一条为例，手动计算就是：<span class="math inline">\(1*log0.3 + 0*log0.7=-1.2040\)</span> 也就是损失函数。</p>
<p>另外torch中另一个相关的 损失函数是<code>BCEWithLogitsLoss</code> ，这个其实就是sigmoid+BCELoss 将sigmoid操作加进去了。</p>
<p>既然已经有了cross entropy, 为什么还要专门多一个binary的呢，我个人在多标签分类中用BCELoss比较多一些(Multi-label) 。</p>
<p>在多标签分类中，每一类其实都是一个二分类任务(即<strong>是否</strong>将其分为该类别)，这种情况下，用BCELoss是最合适的。</p>
<p>举个例子：文本话题分类中，一句话可能有多个topic ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两条数据，5个标签的score</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment"># tensor([[ 0.9992, -0.0551, -0.7823,  2.4850, -0.5739],</span></span><br><span class="line"><span class="comment">#        [-1.1502, -1.8042, -0.6534,  0.5632, -0.9339]])</span></span><br><span class="line"></span><br><span class="line">x = torch.sigmoid(x) <span class="comment"># 得到每一类标签的概率值</span></span><br><span class="line"><span class="comment"># tensor([[0.7309, 0.4862, 0.3138, 0.9231, 0.3603],</span></span><br><span class="line"><span class="comment">#        [0.2404, 0.1413, 0.3422, 0.6372, 0.2821]])</span></span><br><span class="line"></span><br><span class="line">y =  torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]], </span><br><span class="line">                  dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(F.binary_cross_entropy(x, y, reduction=<span class="string">&#x27;none&#x27;</span>))</span><br><span class="line"><span class="comment">#tensor([[1.3127, 0.7211, 1.1589, 2.5650, 0.4468],</span></span><br><span class="line"><span class="comment">#        [1.4253, 1.9566, 0.4189, 1.0139, 1.2654]])</span></span><br><span class="line"><span class="comment"># 每条数据中，每个二分类的损失</span></span><br><span class="line"><span class="comment"># 如果要得到平均损失，直接reduction=&#x27;mean&#x27;即可</span></span><br></pre></td></tr></table></figure>
<p>另外在计算损失函数的时候，有时候会有mask的需求，比如在文本生成中，句子长度不同，需要做padding，因此padding部分的loss是无效的，不能参加运算，因此，需要把mask部分去掉，一般使用 torch的<code>masked_select</code>即可，举例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">loss = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">loss.required_grad_()</span><br><span class="line"><span class="comment">#tensor([[-0.1425, -0.9712, -0.5876, -0.6469,  0.1424],</span></span><br><span class="line"><span class="comment">#        [ 1.0024,  0.6287,  1.5695, -1.0321,  0.1236]], requires_grad=True)</span></span><br><span class="line">mask = torch.tensor( [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]], </span><br><span class="line">                    dtype=torch.<span class="built_in">bool</span>)</span><br><span class="line">loss.selected_mask(mask)</span><br><span class="line"><span class="comment"># tensor([-0.1425, -0.9712, -0.5876, -0.6469,  1.0024,  0.6287,  1.5695],</span></span><br><span class="line"><span class="comment">#       grad_fn=&lt;MaskedSelectBackward&gt;)</span></span><br><span class="line"><span class="comment"># 这样就将mask为true的选取出来了，后续可以求和或者平均</span></span><br></pre></td></tr></table></figure>
<p>总结一下的话，这三个分类loss之间关系很大，本质都是从似然函数得出的。其中NLLLoss是最基础，当然用的也较少。不如直接使用Cross Entropy，再到多标签分类的BinaryCE等。</p>

      
    </div>


    

    
	    <div class="article-footer-copyright">
<!--<p>本文作者: shomy 发表于 <a href="http://shomy.top" target="_blank">个人博客</a></p> -->
<p>
本文标题: PyTorch中NLLLoss|CrossEntropy|BCELoss记录<br/>
</p>
<p>
发布时间: 2021-05-21, 21:07:55<br/>
<p>
<p>
最后更新: 2021-12-16, 23:11:45<br/>
<p>
本文链接: <a href="/2021/05/21/torch-loss/" target="_blank">http://shomy.top/2021/05/21/torch-loss/</a>
</p>
<p>非商业转载请注明作者及出处。商业转载请联系<a href="mailto:shomyliu@gmail.com">作者</a>本人。</p>
</div>

    

    <footer class="article-footer">
      
        <a href="http://shomy.top/2021/05/21/torch-loss/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/" rel="tag">machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li></ul>

    </footer>
    
	    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/01/05/torch-ddp-intro/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Pytorch DDP分布式训练介绍
        
      </div>
    </a>
  
  
    <a href="/2021/05/15/2020-review/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">2020</div>
    </a>
  
</nav>

  
</article>




<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
<script>
  var disqus_shortname = 'shomy';
  
  var disqus_url = 'http://shomy.top/2021/05/21/torch-loss/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  }());
</script>
</section>




</section>
      </div>
    </div>
    
    
<a id="rocket" href="#top" ></a>

<script src="https://cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script>


  <script src="http://cdn.htliu.cn/static/js/leancloud.js"></script>
<script>AV.initialize("k883AKdzaz5jccQnH1eIrG2r-gzGzoHsz", "4HBQOmslEp3VdwK1SGL8vB9Y");</script>

<script src="/js/Counter.js"></script>





<script id="dsq-count-scr" src="//shomy.disqus.com/count.js" async></script>
 


  <script src="https://cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js"></script>
  
<script src="/js/fancybox.js"></script>




  
<script src="/js/search.js"></script>



<script src="/js/script.js"></script>



  </div>
</body>
</html>
